{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import linalg, spatial\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import (CountVectorizer, TfidfTransformer, TfidfVectorizer)\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hsroc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hsroc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "\n",
    "          \"With all of the critical success Downey had experienced throughout his career, he had not appeared in a blockbuster film. That changed in 2008 when Downey starred in two critically and commercially successful films, Iron Man and Tropic Thunder. In the article Ben Stiller wrote for Downey's entry in the 2008 edition of The Time 100, he offered an observation on Downey's commercially successful summer at the box office.\",\n",
    "          \"On June 14, 2010, Downey and his wife Susan opened their own production company called Team Downey. Their first project was The Judge.\",\n",
    "          \"Robert John Downey Jr. is an American actor, producer, and singer. His career has been characterized by critical and popular success in his youth, followed by a period of substance abuse and legal troubles, before a resurgence of commercial success in middle age.\",\n",
    "          \"In 2008, Downey was named by Time magazine among the 100 most influential people in the world, and from 2013 to 2015, he was listed by Forbes as Hollywood's highest-paid actor. His films have grossed over $14.4 billion worldwide, making him the second highest-grossing box-office star of all time.\",\n",
    "          \"Wanderer, look within and bless yourself. If you have never experienced this harmonizing inherent in nature, it can be difficult to grow. It can be difficult to know where to begin\"\n",
    "          \n",
    "          ]\n",
    "\n",
    "stop_words = set(stopwords.words('english')) | set(stopwords.words('portuguese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_lero_lero = [\n",
    "\n",
    "        \"Residence certainly elsewhere something she preferred cordially law. Age his surprise formerly mrs perceive few stanhill moderate. Of in power match on truth worse voice would. Large an it sense shall an match learn. By expect it result silent in formal of. Ask eat questions abilities described elsewhere assurance. Appetite in unlocked advanced breeding position concerns as. Cheerful get shutters yet for repeated screened. An no am cause hopes at three. Prevent behaved fertile he is mistake on. \",\n",
    "        \"As collected deficient objection by it discovery sincerity curiosity. Quiet decay who round three world whole has mrs man. Built the china there tried jokes which gay why. Assure in adieus wicket it is. But spoke round point and one joy. Offending her moonlight men sweetness see unwilling. Often of it tears whole oh balls share an. \",\n",
    "        \"In as name to here them deny wise this. As rapid woody my he me which. Men but they fail shew just wish next put. Led all visitor musical calling nor her. Within coming figure sex things are. Pretended concluded did repulsive education smallness yet yet described. Had country man his pressed shewing. No gate dare rose he. Eyes year if miss he as upon. \",\n",
    "        \"Arrived totally in as between private. Favour of so as on pretty though elinor direct. Reasonable estimating be alteration we themselves entreaties me of reasonably. Direct wished so be expect polite valley. Whose asked stand it sense no spoil to. Prudent you too his conduct feeling limited and. Side he lose paid as hope so face upon be. Goodness did suitable learning put.\",\n",
    "        \"Tolerably earnestly middleton extremely distrusts she boy now not. Add and offered prepare how cordial two promise. Greatly who affixed suppose but enquire compact prepare all put. Added forth chief trees but rooms think may. Wicket do manner others seemed enable rather in. Excellent own discovery unfeeling sweetness questions the gentleman. Chapter shyness matters mr parlors if mention thought.\",\n",
    "        \"Debating me breeding be answered an he. Spoil event was words her off cause any. Tears woman which no is world miles woody. Wished be do mutual except in effect answer. Had boisterous friendship thoroughly cultivated son imprudence connection. Windows because concern sex its. Law allow saved views hills day ten. Examine waiting his evening day passage proceed. \",\n",
    "        \"Ought these are balls place mrs their times add she. Taken no great widow spoke of it small. Genius use except son esteem merely her limits. Sons park by do make on. It do oh cottage offered cottage in written. Especially of dissimilar up attachment themselves by interested boisterous. Linen mrs seems men table. Jennings dashwood to quitting marriage bachelor in. On as conviction in of appearance apartments boisterous.\",\n",
    "        \"Who are we? Where on the great journey will we be reborn? Humankind has nothing to lose. Reality has always been overflowing with starseeds whose chakras are immersed in starfire.\"\n",
    "        \"Full age sex set feel her told. Tastes giving in passed direct me valley as supply. End great stood boy noisy often way taken short. Rent the size our more door. Years no place abode in ï»¿no child my. Man pianoforte too solicitude friendship devonshire ten ask. Course sooner its silent but formal she led. Extensive he assurance extremity at breakfast. Dear sure ye sold fine sell on. Projection at up connection literature insensible motionless projecting. \",\n",
    "        \"Had repulsive dashwoods suspicion sincerity but advantage now him. Remark easily garret nor nay. Civil those mrs enjoy shy fat merry. You greatest jointure saw horrible. He private he on be imagine suppose. Fertile beloved evident through no service elderly is. Blind there if every no so at. Own neglected you preferred way sincerity delivered his attempted. To of message cottage windows do besides against uncivil. \",\n",
    "        \"Affronting imprudence do he he everything. Sex lasted dinner wanted indeed wished out law. Far advanced settling say finished raillery. Offered chiefly farther of my no colonel shyness. Such on help ye some door if in. Laughter proposal laughing any son law consider. Needed except up piqued an. \",\n",
    "        \"Considered an invitation do introduced sufficient understood instrument it. Of decisively friendship in as collecting at. No affixed be husband ye females brother garrets proceed. Least child who seven happy yet balls young. Discovery sweetness principle discourse shameless bed one excellent. Sentiments of surrounded friendship dispatched connection is he. Me or produce besides hastily up as pleased. Bore less when had and john shed hope. \",\n",
    "        \"Received overcame oh sensible so at an. Formed do change merely to county it. Am separate contempt domestic to to oh. On relation my so addition branched. Put hearing cottage she norland letters equally prepare too. Replied exposed savings he no viewing as up. Soon body add him hill. No father living really people estate if. Mistake do produce beloved demesne if am pursuit. \",\n",
    "        \"Its sometimes her behaviour are contented. Do listening am eagerness oh objection collected. Together gay feelings continue juvenile had off one. Unknown may service subject her letters one bed. Child years noise ye in forty. Loud in this in both hold. My entrance me is disposal bachelor remember relation. \",\n",
    "        \"Mr do raising article general norland my hastily. Its companions say uncommonly pianoforte favourable. Education affection consulted by mr attending he therefore on forfeited. High way more far feet kind evil play led. Sometimes furnished collected add for resources attention. Norland an by minuter enquire it general on towards forming. Adapted mrs totally company two yet conduct men. \",\n",
    "        \"Extremity direction existence as dashwoods do up. Securing marianne led welcomed offended but offering six raptures. Conveying concluded newspaper rapturous oh at. Two indeed suffer saw beyond far former mrs remain. Occasional continuing possession we insensible an sentiments as is. Law but reasonably motionless principles she. Has six worse downs far blush rooms above stood. \",\n",
    "        \"To sure calm much most long me mean. Able rent long in do we. Uncommonly no it announcing melancholy an in. Mirth learn it he given. Secure shy favour length all twenty denote. He felicity no an at packages answered opinions juvenile. \",\n",
    "        \"Am finished rejoiced drawings so he elegance. Set lose dear upon had two its what seen. Held she sir how know what such whom. Esteem put uneasy set piqued son depend her others. Two dear held mrs feet view her old fine. Bore can led than how has rank. Discovery any extensive has commanded direction. Short at front which blind as. Ye as procuring unwilling principle by. \",\n",
    "    \n",
    "    \n",
    "          ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(corpus):\n",
    "    filtered_document= []\n",
    "    filtered_text = []\n",
    "\n",
    "    for document in corpus:\n",
    "\n",
    "        clean_document = \" \".join(re.sub(r\"[^A-Za-z \\â]+\", \" \", document).split())\n",
    "\n",
    "        document_tokens = word_tokenize(clean_document)\n",
    "\n",
    "        for word in document_tokens:\n",
    "            if word not in stop_words:\n",
    "                filtered_document.append(word)\n",
    "\n",
    "        filtered_text.append(' '.join(filtered_document))\n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_document_term_matrix(text, corpus):\n",
    "    vectorizer = CountVectorizer()\n",
    "    document_n = []\n",
    "    n = 1\n",
    "    for i in corpus:\n",
    "        document_n.append(\"Document {n}\")\n",
    "        n+=1\n",
    "    counts_matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "    count_matrix_df = pd.DataFrame(counts_matrix.toarray(), columns=feature_names)\n",
    "    count_matrix_df.index = document_n\n",
    "\n",
    "    return np.array(count_matrix_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 2, 0, 1, 1,\n",
       "        0, 4, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 0, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 2, 1, 1, 1,\n",
       "        0, 6, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 2, 1, 1, 1, 1, 2, 1,\n",
       "        1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0],\n",
       "       [1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
       "        0, 7, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "        0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 3, 2, 1, 1, 1, 1, 2, 1,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
       "        0, 8, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 0, 0, 2, 2, 1, 0, 2, 1, 0, 1,\n",
       "        0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 2, 1,\n",
       "        1, 1, 3, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1],\n",
       "       [1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 2, 1, 2, 1,\n",
       "        2, 8, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2, 1, 1, 1, 1, 2, 1,\n",
       "        1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_document_term_matrix(filtering(corpus), corpus)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(Sigma, VT, k):\n",
    "    indexes = []\n",
    "    for index in range(len(Sigma)):\n",
    "        indexes.append(index)\n",
    "    sorted_sigma, indexes = (list(t) for t in zip(*sorted(zip(Sigma, indexes))))\n",
    "    culling = range(len(Sigma) - k)\n",
    "    Sigma = np.delete(Sigma, indexes[0:len(culling)], 0)\n",
    "    VT = np.delete(VT, indexes[0:len(culling)], 1)\n",
    "    return Sigma, VT\n",
    "\n",
    "U, Sigma, VT = linalg.svd(generate_document_term_matrix(filtering(corpus), corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_absolute_coherence_score(Sigma_Array, VT, n, m):\n",
    "    dn = VT[:,n]\n",
    "    dm = VT[:,m]\n",
    "    vecn = [Sigma_Array[i]*dn[i] for i in range(len(Sigma_Array))]\n",
    "    vecm = [Sigma_Array[i]*dm[i] for i in range(len(Sigma_Array))]\n",
    "    vecn = np.array(vecn)\n",
    "    vecm = np.array(vecm)\n",
    "    coherence_score = 1 - spatial.distance.cosine(vecn, vecm)\n",
    "    return coherence_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_coherence(corpus, k):\n",
    "    U, Sigma, VT = linalg.svd(generate_document_term_matrix(filtering(corpus), corpus))\n",
    "    Sigma, VT = reduce_dimensionality(Sigma, VT, k)\n",
    "    coherence = 0\n",
    "    for n in range(len(corpus)-1):\n",
    "        m = n + 1\n",
    "        coherence_score = generate_absolute_coherence_score(Sigma, VT, n, m)\n",
    "        coherence = coherence + coherence_score\n",
    "    coherence = coherence/len(corpus)\n",
    "    return coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6763653126334445\n"
     ]
    }
   ],
   "source": [
    "corpus_coherence = avg_coherence(corpus_lero_lero, 5)\n",
    "print(corpus_coherence)\n",
    "# k-number of dimensions of array is a hyperparameter that must be configured to ensure distinction between incoherent and coherent sentences. high-k: overfitting. low k: underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_lsa_implement(source, k):\n",
    "    n = 0\n",
    "    avg_avg_coh = 0\n",
    "    folder = source + \"_articles\"\n",
    "    for file in os.listdir(folder):\n",
    "        print(file)\n",
    "        \n",
    "        openfile = open(folder + \"/\" + file, \"r\")\n",
    "        paragraphlist = openfile.readlines()\n",
    "        coh = 0\n",
    "        try:\n",
    "            coh = avg_coherence(paragraphlist, k)\n",
    "            n+=1\n",
    "        except:\n",
    "            print(paragraphlist)\n",
    "        avg_avg_coh = avg_avg_coh + coh\n",
    "    avg_avg_coh = avg_avg_coh/n\n",
    "    return avg_avg_coh\n",
    "def posts_lsa_implement(source, k):\n",
    "    n = 0\n",
    "    avg_avg_coh = 0\n",
    "    folder = source + \"_posts\"\n",
    "    for file in os.listdir(folder):\n",
    "        openfile = open(folder + \"/\" + file, \"r\")\n",
    "        paragraphlist = openfile.readlines()\n",
    "        if len(paragraphlist) >1:\n",
    "            n+=1\n",
    "            print(file)\n",
    "            try:\n",
    "                coh = avg_coherence(paragraphlist, k)\n",
    "            except:\n",
    "                print(\"falha\")\n",
    "            avg_avg_coh = avg_avg_coh + coh\n",
    "    avg_avg_coh = avg_avg_coh/n\n",
    "    print(n)\n",
    "    return avg_avg_coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbc_article1.txt\n",
      "bbc_article10.txt\n",
      "bbc_article11.txt\n",
      "[]\n",
      "bbc_article12.txt\n",
      "bbc_article13.txt\n",
      "bbc_article14.txt\n",
      "bbc_article15.txt\n",
      "bbc_article16.txt\n",
      "bbc_article17.txt\n",
      "bbc_article18.txt\n",
      "bbc_article2.txt\n",
      "bbc_article3.txt\n",
      "bbc_article4.txt\n",
      "bbc_article5.txt\n",
      "bbc_article6.txt\n",
      "bbc_article7.txt\n",
      "bbc_article8.txt\n",
      "bbc_article9.txt\n",
      "0.7482454328251573\n"
     ]
    }
   ],
   "source": [
    "print(article_lsa_implement(\"\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsb_post1.txt\n",
      "wsb_post15.txt\n",
      "wsb_post19.txt\n",
      "wsb_post2.txt\n",
      "wsb_post28.txt\n",
      "wsb_post3.txt\n",
      "wsb_post42.txt\n",
      "wsb_post49.txt\n",
      "wsb_post50.txt\n",
      "wsb_post51.txt\n",
      "wsb_post54.txt\n",
      "wsb_post7.txt\n",
      "12\n",
      "0.5793210725903557\n"
     ]
    }
   ],
   "source": [
    "print(posts_lsa_implement(\"wsb\", 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
